name: Test LLM Models on Main

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  test-llama-7b:
    runs-on: self-hosted
    steps:
    - name: clone repository
      run: |
        cd /workspace
        rm -rf ./multi-lora-fine-tune
        git clone git@github.com:TUDB-Labs/multi-lora-fine-tune.git
    - name: finetune llama-7b
      run: |
        cd /workspace/multi-lora-fine-tune
        python mlora.py --base_model /data/llama-7b-hf --config ./config/finetune.json --load_8bit

    - name: test inference with lora
      run: |
        cd /workspace/multi-lora-fine-tune
        python ci_script.py "llama" "/data/llama-7b-hf" "./lora_0" "Say something."

  test-vicuna-7b:
    runs-on: self-hosted
    steps:
    - name: clone repository
      run: |
        cd /workspace
        rm -rf ./multi-lora-fine-tune
        git clone git@github.com:TUDB-Labs/multi-lora-fine-tune.git

    - name: finetune vicuna-7b-v1.1
      run: |
        cd /workspace/multi-lora-fine-tune
        python mlora.py --base_model /data/vicuna-7b-v1.1 --config ./config/finetune.json --load_8bit

    - name: test inference with lora
      run: |
        cd /workspace/multi-lora-fine-tune
        python ci_script.py "llama" "/data/vicuna-7b-v1.1" "./lora_0" "Say something."

test-chatglm2:
  runs-on: self-hosted
  steps:
    - name: clone repository
      run: |
        cd /workspace
        rm -rf ./multi-lora-fine-tune
        git clone git@github.com:TUDB-Labs/multi-lora-fine-tune.git

    - name: finetune chatglm2-6b
      run: |
        cd /workspace/multi-lora-fine-tune
        python mlora.py --base_model /data/THUDM/chatglm2-6b --config ./config/finetune_chatglm.json --model_type chatglm --load_8bit

    - name: test inference with lora
      run: |
        cd /workspace/multi-lora-fine-tune
        python ci_script.py "chatglm" "/data/THUDM/chatglm2-6b" "./lora_0" "Say something."