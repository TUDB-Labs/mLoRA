name: Test LLM Models on Dev

on:
  push:
    except:
      branches:
        - 'main'

jobs:
  test-llama-7b:
    runs-on: self-hosted
    steps:
      - name: Get Branch Name
        id: branch_name
        run: echo "::set-output name=branch::${GITHUB_REF#refs/heads/}"

      - name: clone repository
        run: |
          current_branch="${{ steps.branch_name.outputs.branch }}"
          cd /workspace
          rm -rf ./multi-lora-fine-tune
          git clone -b $current_branch --single-branch git@github.com:TUDB-Labs/multi-lora-fine-tune.git

      - name: finetune llama-7b
        run: |
          cd /workspace/multi-lora-fine-tune
          python mlora.py --base_model /data/llama-7b-hf --config ./config/dummy.json --load_8bit

      - name: test inference with lora
        run: |
          cd /workspace/multi-lora-fine-tune
          python .github/workflows/ci_script.py "llama" "/data/llama-7b-hf" "./lora_1" "What is m-LoRA?"

  test-vicuna-7b:
    runs-on: self-hosted
    steps:
      - name: Get Branch Name
        id: branch_name
        run: echo "::set-output name=branch::${GITHUB_REF#refs/heads/}"

      - name: clone repository
        run: |
          current_branch="${{ steps.branch_name.outputs.branch }}"
          cd /workspace
          rm -rf ./multi-lora-fine-tune
          git clone -b $current_branch --single-branch git@github.com:TUDB-Labs/multi-lora-fine-tune.git

      - name: finetune vicuna-7b-v1.1
        run: |
          cd /workspace/multi-lora-fine-tune
          python mlora.py --base_model /data/vicuna-7b-v1.1 --config ./config/finetune.json --load_8bit

      - name: test inference with lora
        run: |
          cd /workspace/multi-lora-fine-tune
          python .github/workflows/ci_script.py "llama" "/data/vicuna-7b-v1.1" "./lora_0" "Say something."

  test-chatglm2:
    runs-on: self-hosted
    steps:
      - name: Get Branch Name
        id: branch_name
        run: echo "::set-output name=branch::${GITHUB_REF#refs/heads/}"

      - name: clone repository
        run: |
          current_branch="${{ steps.branch_name.outputs.branch }}"
          cd /workspace
          rm -rf ./multi-lora-fine-tune
          git clone -b $current_branch --single-branch git@github.com:TUDB-Labs/multi-lora-fine-tune.git

      - name: finetune chatglm2-6b
        run: |
          cd /workspace/multi-lora-fine-tune
          python mlora.py --base_model /data/THUDM/chatglm2-6b --config ./config/finetune_chatglm.json --model_type chatglm --load_8bit

      - name: test inference with lora
        run: |
          cd /workspace/multi-lora-fine-tune
          python .github/workflows/ci_script.py "chatglm" "/data/THUDM/chatglm2-6b" "./lora_0" "Say something."

  test-mixlora:
    runs-on: self-hosted
    steps:
      - name: Get Branch Name
        id: branch_name
        run: echo "::set-output name=branch::${GITHUB_REF#refs/heads/}"

      - name: clone repository
        run: |
          current_branch="${{ steps.branch_name.outputs.branch }}"
          cd /workspace
          rm -rf ./multi-lora-fine-tune
          git clone -b $current_branch --single-branch git@github.com:TUDB-Labs/multi-lora-fine-tune.git

      - name: finetune llama-7b with mixlora
        run: |
          cd /workspace/multi-lora-fine-tune
          python mlora.py --base_model /data/llama-7b-hf --config ./config/finetune_mixlora.json --load_8bit
